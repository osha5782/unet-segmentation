{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unet.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yasohasakii/unet-segmentation/blob/master/Unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXF0mNZVGrRc",
        "colab_type": "code",
        "outputId": "0af2ecc4-03e3-41d5-89c2-72c5fc497873",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!rm -rf *\n",
        "!git clone https://github.com/yasohasakii/unet-segmentation.git\n",
        "!cp -r unet-segmentation/* ./\n",
        "!rm -rf unet-segmentation/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'unet-segmentation'...\n",
            "remote: Enumerating objects: 48, done.\u001b[K\n",
            "remote: Counting objects: 100% (48/48), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 248 (delta 17), reused 0 (delta 0), pack-reused 200\u001b[K\n",
            "Receiving objects: 100% (248/248), 513.93 MiB | 11.28 MiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n",
            "Checking out files: 100% (220/220), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns9YXewiU0H8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau\n",
        "from keras import backend as K\n",
        "from keras.losses import binary_crossentropy\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zX3Mf5qgxln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#metrics were from https://www.kaggle.com/meaninglesslives/unet-plus-plus-with-efficientnet-encoder\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred = K.cast(y_pred, 'float32')\n",
        "    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n",
        "    intersection = y_true_f * y_pred_f\n",
        "    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n",
        "    return score\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    smooth = 1.\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = y_true_f * y_pred_f\n",
        "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "    return 1. - score\n",
        "\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "\n",
        "def get_iou_vector(A, B):\n",
        "    # Numpy version    \n",
        "    batch_size = A.shape[0]\n",
        "    metric = 0.0\n",
        "    for batch in range(batch_size):\n",
        "        t, p = A[batch], B[batch]\n",
        "        true = np.sum(t)\n",
        "        pred = np.sum(p)\n",
        "        \n",
        "        # deal with empty mask first\n",
        "        if true == 0:\n",
        "            metric += (pred == 0)\n",
        "            continue\n",
        "        \n",
        "        # non empty mask case.  Union is never empty \n",
        "        # hence it is safe to divide by its number of pixels\n",
        "        intersection = np.sum(t * p)\n",
        "        union = true + pred - intersection\n",
        "        iou = intersection / union\n",
        "        \n",
        "        # iou metrric is a stepwise approximation of the real iou over 0.5\n",
        "        iou = np.floor(max(0, (iou - 0.45)*20)) / 10\n",
        "        \n",
        "        metric += iou\n",
        "        \n",
        "    # teake the average over all images in batch\n",
        "    metric /= batch_size\n",
        "    return metric\n",
        "\n",
        "\n",
        "def my_iou_metric(label, pred):\n",
        "    # Tensorflow version\n",
        "    return tf.py_func(get_iou_vector, [label, pred > 0.5], tf.float64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vcav7XpTWFKJ",
        "colab_type": "code",
        "outputId": "9ecdd346-6483-4a40-b999-7bccfc7782fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def down_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
        "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n",
        "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
        "    b = keras.layers.BatchNormalization()(c)\n",
        "    d = keras.layers.Dropout(0.25)(b)\n",
        "    p = keras.layers.MaxPool2D((2, 2), (2, 2))(d)\n",
        "    return c, p\n",
        "\n",
        "def up_block(x, skip, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
        "    us = keras.layers.UpSampling2D((2, 2))(x)\n",
        "    concat = keras.layers.Concatenate()([us, skip])\n",
        "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(concat)\n",
        "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
        "    c = keras.layers.BatchNormalization()(c)\n",
        "    return c\n",
        "\n",
        "def bottleneck(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
        "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n",
        "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
        "    return c\n",
        "def UNet():\n",
        "    f = [64, 128, 256, 512, 1024]\n",
        "    inputs = keras.layers.Input((512, 512, 3))\n",
        "    \n",
        "    p0 = inputs\n",
        "    c1, p1 = down_block(p0, f[0]) #1024 -> 512\n",
        "    c2, p2 = down_block(p1, f[1]) #512 -> 256\n",
        "    c3, p3 = down_block(p2, f[2]) #256 -> 128\n",
        "    c4, p4 = down_block(p3, f[3]) #128 -> 64\n",
        "    \n",
        "    bn = bottleneck(p4, f[4])\n",
        "    \n",
        "    u1 = up_block(bn, c4, f[3]) #64 -> 128\n",
        "    u2 = up_block(u1, c3, f[2]) #128 -> 256\n",
        "    u3 = up_block(u2, c2, f[1]) #256 -> 512\n",
        "    u4 = up_block(u3, c1, f[0]) #512 -> 1024\n",
        "    \n",
        "    out = keras.layers.Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(u4)\n",
        "    outputs = keras.layers.Reshape((512,512))(out)\n",
        "    model = keras.models.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "model = UNet()\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.0001), loss=bce_dice_loss, metrics=[my_iou_metric])\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 512, 512, 64) 1792        input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 512, 512, 64) 36928       conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 512, 512, 64) 256         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 512, 512, 64) 0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2D)  (None, 256, 256, 64) 0           dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 256, 256, 128 73856       max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 256, 256, 128 147584      conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 256, 256, 128 512         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 256, 256, 128 0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling2D) (None, 128, 128, 128 0           dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 128, 128, 256 295168      max_pooling2d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 128, 128, 256 590080      conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 128, 128, 256 1024        conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 128, 128, 256 0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling2D) (None, 64, 64, 256)  0           dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 64, 64, 512)  1180160     max_pooling2d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 64, 64, 512)  2359808     conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 64, 64, 512)  2048        conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 64, 64, 512)  0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling2D) (None, 32, 32, 512)  0           dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 32, 32, 1024) 4719616     max_pooling2d_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 32, 32, 1024) 9438208     conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_9 (UpSampling2D)  (None, 64, 64, 1024) 0           conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 64, 64, 1536) 0           up_sampling2d_9[0][0]            \n",
            "                                                                 conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 64, 64, 512)  7078400     concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 64, 64, 512)  2359808     conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 64, 64, 512)  2048        conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_10 (UpSampling2D) (None, 128, 128, 512 0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 128, 128, 768 0           up_sampling2d_10[0][0]           \n",
            "                                                                 conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 128, 128, 256 1769728     concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 128, 128, 256 590080      conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 128, 128, 256 1024        conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_11 (UpSampling2D) (None, 256, 256, 256 0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 256, 256, 384 0           up_sampling2d_11[0][0]           \n",
            "                                                                 conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 256, 256, 128 442496      concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 256, 256, 128 147584      conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 256, 256, 128 512         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_12 (UpSampling2D) (None, 512, 512, 128 0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 512, 512, 192 0           up_sampling2d_12[0][0]           \n",
            "                                                                 conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 512, 512, 64) 110656      concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 512, 512, 64) 36928       conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 512, 512, 64) 256         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 512, 512, 1)  65          batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 512, 512)     0           conv2d_57[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 31,386,625\n",
            "Trainable params: 31,382,785\n",
            "Non-trainable params: 3,840\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N74gPOQWabV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataGen(keras.utils.Sequence):\n",
        "    def __init__(self, path, batch_size=1, image_size=512):\n",
        "        self.path = path\n",
        "        self.batch_size = batch_size\n",
        "        self.image_size = image_size\n",
        "        files = os.listdir(self.path)\n",
        "        files = [os.path.join(self.path,x) for x in files]\n",
        "        self.trains, self.vals = train_test_split(files, test_size=0.1, random_state=42)\n",
        "    \n",
        "    def generate(self,files): \n",
        "        random.shuffle(files)\n",
        "        while True:\n",
        "            image_batch = np.zeros([self.batch_size,self.image_size,self.image_size,3])\n",
        "            label_batch = np.zeros([self.batch_size,self.image_size,self.image_size])\n",
        "            index = random.randint(0,len(files)-self.batch_size)\n",
        "            for i,img in enumerate(files[index:index+self.batch_size]):\n",
        "        \n",
        "                ## Reading Image\n",
        "                image = Image.open(img)\n",
        "                image = image.resize((self.image_size, self.image_size))\n",
        "                image = np.array(image)\n",
        "        \n",
        "                _mask_image = Image.open(img.replace('raw','label'))\n",
        "                _mask_image = _mask_image.convert('L')\n",
        "                _mask_image = _mask_image.resize((self.image_size, self.image_size)) #128x128\n",
        "                mask = np.array(_mask_image)\n",
        "            \n",
        "                ## Normalizaing \n",
        "                image = image/255.0\n",
        "                mask = mask/255.0\n",
        "                # print(np.max(mask))\n",
        "            image_batch[i]=image\n",
        "            label_batch[i]=mask\n",
        "        \n",
        "            yield image_batch, label_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydtte7OEYwKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path = '/content/raw'\n",
        "batch_size= 1\n",
        "gen = DataGen( train_path, image_size=512, batch_size=batch_size)\n",
        "train_gen = gen.generate(gen.trains)\n",
        "val_gen = gen.generate(gen.vals)\n",
        "\n",
        "\n",
        "train_steps = len(gen.trains)//batch_size\n",
        "valid_steps = len(gen.vals)//batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pbR9ggDpkr3",
        "colab_type": "code",
        "outputId": "c9191b8a-449a-4e0a-9b91-ecaa596e2d55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_checkpoint = ModelCheckpoint('unet_membrane.h5', monitor='val_my_iou_metric',mode='max',verbose=1,save_best_only=True,save_weights_only=True)\n",
        "changelr = ReduceLROnPlateau(monitor = 'val_my_iou_metric',\n",
        "                patience=5,mode = 'max',\n",
        "                verbose = 1,\n",
        "                factor = 0.6,\n",
        "                min_lr = 0.00001)\n",
        "h = model.fit_generator(train_gen,steps_per_epoch=train_steps,epochs=30,\n",
        "                    callbacks=[model_checkpoint,changelr],\n",
        "                    validation_data = val_gen,validation_steps = valid_steps)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/30\n",
            "90/90 [==============================] - 83s 928ms/step - loss: 1.3390 - my_iou_metric: 0.0278 - val_loss: 2.5443 - val_my_iou_metric: 0.0100\n",
            "\n",
            "Epoch 00001: val_my_iou_metric improved from -inf to 0.01000, saving model to unet_membrane.h5\n",
            "Epoch 2/30\n",
            "90/90 [==============================] - 75s 837ms/step - loss: 1.0378 - my_iou_metric: 0.1589 - val_loss: 1.0812 - val_my_iou_metric: 0.0000e+00\n",
            "\n",
            "Epoch 00002: val_my_iou_metric did not improve from 0.01000\n",
            "Epoch 3/30\n",
            "90/90 [==============================] - 75s 835ms/step - loss: 0.9533 - my_iou_metric: 0.1989 - val_loss: 1.1013 - val_my_iou_metric: 0.0000e+00\n",
            "\n",
            "Epoch 00003: val_my_iou_metric did not improve from 0.01000\n",
            "Epoch 4/30\n",
            "90/90 [==============================] - 75s 836ms/step - loss: 0.8887 - my_iou_metric: 0.2244 - val_loss: 0.8819 - val_my_iou_metric: 0.1000\n",
            "\n",
            "Epoch 00004: val_my_iou_metric improved from 0.01000 to 0.10000, saving model to unet_membrane.h5\n",
            "Epoch 5/30\n",
            "90/90 [==============================] - 76s 840ms/step - loss: 0.8492 - my_iou_metric: 0.2522 - val_loss: 0.7028 - val_my_iou_metric: 0.1200\n",
            "\n",
            "Epoch 00005: val_my_iou_metric improved from 0.10000 to 0.12000, saving model to unet_membrane.h5\n",
            "Epoch 6/30\n",
            "90/90 [==============================] - 76s 839ms/step - loss: 0.8054 - my_iou_metric: 0.2933 - val_loss: 0.7368 - val_my_iou_metric: 0.1800\n",
            "\n",
            "Epoch 00006: val_my_iou_metric improved from 0.12000 to 0.18000, saving model to unet_membrane.h5\n",
            "Epoch 7/30\n",
            "90/90 [==============================] - 76s 840ms/step - loss: 0.7495 - my_iou_metric: 0.3189 - val_loss: 1.5106 - val_my_iou_metric: 0.0200\n",
            "\n",
            "Epoch 00007: val_my_iou_metric did not improve from 0.18000\n",
            "Epoch 8/30\n",
            "90/90 [==============================] - 76s 839ms/step - loss: 0.5591 - my_iou_metric: 0.4233 - val_loss: 0.8136 - val_my_iou_metric: 0.1400\n",
            "\n",
            "Epoch 00008: val_my_iou_metric did not improve from 0.18000\n",
            "Epoch 9/30\n",
            "90/90 [==============================] - 75s 839ms/step - loss: 0.5907 - my_iou_metric: 0.4344 - val_loss: 0.8898 - val_my_iou_metric: 0.0100\n",
            "\n",
            "Epoch 00009: val_my_iou_metric did not improve from 0.18000\n",
            "Epoch 10/30\n",
            "90/90 [==============================] - 75s 834ms/step - loss: 0.6124 - my_iou_metric: 0.3933 - val_loss: 0.6445 - val_my_iou_metric: 0.3000\n",
            "\n",
            "Epoch 00010: val_my_iou_metric improved from 0.18000 to 0.30000, saving model to unet_membrane.h5\n",
            "Epoch 11/30\n",
            "90/90 [==============================] - 75s 834ms/step - loss: 0.4986 - my_iou_metric: 0.4622 - val_loss: 0.7825 - val_my_iou_metric: 0.1200\n",
            "\n",
            "Epoch 00011: val_my_iou_metric did not improve from 0.30000\n",
            "Epoch 12/30\n",
            "90/90 [==============================] - 75s 830ms/step - loss: 0.5858 - my_iou_metric: 0.4144 - val_loss: 0.8559 - val_my_iou_metric: 0.1000\n",
            "\n",
            "Epoch 00012: val_my_iou_metric did not improve from 0.30000\n",
            "Epoch 13/30\n",
            "90/90 [==============================] - 75s 830ms/step - loss: 0.5192 - my_iou_metric: 0.4656 - val_loss: 0.6434 - val_my_iou_metric: 0.3100\n",
            "\n",
            "Epoch 00013: val_my_iou_metric improved from 0.30000 to 0.31000, saving model to unet_membrane.h5\n",
            "Epoch 14/30\n",
            "90/90 [==============================] - 75s 831ms/step - loss: 0.4868 - my_iou_metric: 0.5033 - val_loss: 0.5657 - val_my_iou_metric: 0.3200\n",
            "\n",
            "Epoch 00014: val_my_iou_metric improved from 0.31000 to 0.32000, saving model to unet_membrane.h5\n",
            "Epoch 15/30\n",
            "90/90 [==============================] - 75s 829ms/step - loss: 0.4427 - my_iou_metric: 0.5511 - val_loss: 0.5682 - val_my_iou_metric: 0.2500\n",
            "\n",
            "Epoch 00015: val_my_iou_metric did not improve from 0.32000\n",
            "Epoch 16/30\n",
            "90/90 [==============================] - 75s 831ms/step - loss: 0.3922 - my_iou_metric: 0.5611 - val_loss: 0.7547 - val_my_iou_metric: 0.0900\n",
            "\n",
            "Epoch 00016: val_my_iou_metric did not improve from 0.32000\n",
            "Epoch 17/30\n",
            "90/90 [==============================] - 75s 829ms/step - loss: 0.4348 - my_iou_metric: 0.5356 - val_loss: 0.8462 - val_my_iou_metric: 0.0900\n",
            "\n",
            "Epoch 00017: val_my_iou_metric did not improve from 0.32000\n",
            "Epoch 18/30\n",
            "90/90 [==============================] - 75s 829ms/step - loss: 0.3862 - my_iou_metric: 0.5789 - val_loss: 0.5087 - val_my_iou_metric: 0.2500\n",
            "\n",
            "Epoch 00018: val_my_iou_metric did not improve from 0.32000\n",
            "Epoch 19/30\n",
            "90/90 [==============================] - 75s 828ms/step - loss: 0.4280 - my_iou_metric: 0.5422 - val_loss: 0.6710 - val_my_iou_metric: 0.1600\n",
            "\n",
            "Epoch 00019: val_my_iou_metric did not improve from 0.32000\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 5.999999848427251e-05.\n",
            "Epoch 20/30\n",
            "90/90 [==============================] - 74s 826ms/step - loss: 0.3428 - my_iou_metric: 0.6533 - val_loss: 0.6327 - val_my_iou_metric: 0.1800\n",
            "\n",
            "Epoch 00020: val_my_iou_metric did not improve from 0.32000\n",
            "Epoch 21/30\n",
            "90/90 [==============================] - 74s 827ms/step - loss: 0.3345 - my_iou_metric: 0.6433 - val_loss: 0.7486 - val_my_iou_metric: 0.1900\n",
            "\n",
            "Epoch 00021: val_my_iou_metric did not improve from 0.32000\n",
            "Epoch 22/30\n",
            "90/90 [==============================] - 74s 824ms/step - loss: 0.3309 - my_iou_metric: 0.6422 - val_loss: 0.5593 - val_my_iou_metric: 0.2800\n",
            "\n",
            "Epoch 00022: val_my_iou_metric did not improve from 0.32000\n",
            "Epoch 23/30\n",
            "90/90 [==============================] - 74s 826ms/step - loss: 0.3413 - my_iou_metric: 0.6478 - val_loss: 0.4895 - val_my_iou_metric: 0.3000\n",
            "\n",
            "Epoch 00023: val_my_iou_metric did not improve from 0.32000\n",
            "Epoch 24/30\n",
            "90/90 [==============================] - 74s 827ms/step - loss: 0.3478 - my_iou_metric: 0.6456 - val_loss: 0.7174 - val_my_iou_metric: 0.2100\n",
            "\n",
            "Epoch 00024: val_my_iou_metric did not improve from 0.32000\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 3.599999909056351e-05.\n",
            "Epoch 25/30\n",
            "90/90 [==============================] - 74s 826ms/step - loss: 0.3229 - my_iou_metric: 0.6700 - val_loss: 0.5849 - val_my_iou_metric: 0.2400\n",
            "\n",
            "Epoch 00025: val_my_iou_metric did not improve from 0.32000\n",
            "Epoch 26/30\n",
            "90/90 [==============================] - 75s 828ms/step - loss: 0.3426 - my_iou_metric: 0.6611 - val_loss: 0.7711 - val_my_iou_metric: 0.1200\n",
            "\n",
            "Epoch 00026: val_my_iou_metric did not improve from 0.32000\n",
            "Epoch 27/30\n",
            "90/90 [==============================] - 75s 829ms/step - loss: 0.2896 - my_iou_metric: 0.7067 - val_loss: 0.5074 - val_my_iou_metric: 0.3100\n",
            "\n",
            "Epoch 00027: val_my_iou_metric did not improve from 0.32000\n",
            "Epoch 28/30\n",
            "90/90 [==============================] - 75s 828ms/step - loss: 0.2312 - my_iou_metric: 0.7444 - val_loss: 0.4639 - val_my_iou_metric: 0.4600\n",
            "\n",
            "Epoch 00028: val_my_iou_metric improved from 0.32000 to 0.46000, saving model to unet_membrane.h5\n",
            "Epoch 29/30\n",
            "90/90 [==============================] - 74s 826ms/step - loss: 0.2673 - my_iou_metric: 0.7367 - val_loss: 0.7702 - val_my_iou_metric: 0.1800\n",
            "\n",
            "Epoch 00029: val_my_iou_metric did not improve from 0.46000\n",
            "Epoch 30/30\n",
            "90/90 [==============================] - 75s 831ms/step - loss: 0.3083 - my_iou_metric: 0.6844 - val_loss: 0.5732 - val_my_iou_metric: 0.3100\n",
            "\n",
            "Epoch 00030: val_my_iou_metric did not improve from 0.46000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcxesAStv1yG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import glob, cv2\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.load_weights('/content/unet_membrane.h5')\n",
        "\n",
        "def predict(model,image):\n",
        "    image = np.array(image,np.float)/255.0\n",
        "    image = np.expand_dims(image,axis=0)\n",
        "    pred = model.predict(image)[0]\n",
        "    pred = (pred-np.min(pred))/(np.max(pred)-np.min(pred))\n",
        "    pred = cv2.merge([pred,pred,pred])\n",
        "    return pred\n",
        "\n",
        "def plot_result(model,img):\n",
        "    image = Image.open(img)\n",
        "    h,w = image.size\n",
        "    copy = image.resize((1024,1024))\n",
        "    copy = np.array(copy,np.float)\n",
        "    pred = predict(model,copy)\n",
        "    pred = cv2.resize(pred,(h,w))\n",
        "    blend = np.array(image)*pred\n",
        "    blend = np.asarray(blend,np.uint8)\n",
        "    return blend\n",
        "    \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    images = glob.glob('/content/test/*.png')\n",
        "    for image in images:\n",
        "        result = plot_result(model,image)\n",
        "        _image = np.array(Image.open(image))\n",
        "    \n",
        "        plt.figure(figsize=(48,16))\n",
        "        plt.subplot(121)\n",
        "        plt.title('raw_image')\n",
        "        plt.axis('off') \n",
        "        plt.imshow(_image)\n",
        "\n",
        "        plt.subplot(122)\n",
        "        plt.title('model_result')\n",
        "        plt.axis('off') \n",
        "        plt.imshow(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXdQP4d9z2Y9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}